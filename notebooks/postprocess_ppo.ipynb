{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load a Trained Text-based Small Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-29 14:13:29,965\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "/anaconda/envs/zero/lib/python3.9/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:\n",
      "No module named 'vllm._version'\n",
      "  from vllm.version import __version__ as VLLM_VERSION\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers Version: 4.47.1\n",
      "Torch Version: 2.4.0+cu121\n",
      "4 CUDA (12.1) device available\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    pipeline)\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "# from peft import AutoPeftModelForCausalLM, PeftModel\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.lora.request import LoRARequest\n",
    "\n",
    "print(f\"Transformers Version: {transformers.__version__}\")\n",
    "print(f\"Torch Version: {torch.__version__}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"{torch.cuda.device_count()} CUDA ({torch.version.cuda}) device available\")\n",
    "else:\n",
    "    print(\"No CUDA device available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lora_adapters_dir = \"/home/azureuser/localfiles/models/frosty_apple_5d9xqfl2lw\"\n",
    "lora_adapters_dir = \"/home/azureuser/localfiles/models/grpo\"\n",
    "# merged_model_folder = \"/home/azureuser/localfiles/models/frosty_apple_5d9xqfl2lw_merged\"\n",
    "merged_model_folder = '/mnt/compatibility_checkpoints/actor/global_step_224'\n",
    "output_string = 'compatibility_text_based'\n",
    "sft_test_file = f\"{output_string}_sft_test_2894.jsonl\"\n",
    "data_dir = \"/home/azureuser/localfiles/data/polyvore_cp\"\n",
    "\n",
    "save_model = False\n",
    "use_lora_adapter = False\n",
    "base_model = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "# base_model = \"microsoft/Phi-3-mini-128k-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_model:\n",
    "\n",
    "    # Load the base model\n",
    "    # Determine if GPU is available\n",
    "    if torch.cuda.is_available():\n",
    "        device_map = 'cuda'\n",
    "        torch_dtype = torch.float16\n",
    "    else:\n",
    "        device_map = 'cpu'\n",
    "        torch_dtype = torch.bfloat16\n",
    "\n",
    "    model_kwargs = {\n",
    "        \"use_cache\": False,\n",
    "        \"trust_remote_code\": True,\n",
    "        \"attn_implementation\": \"flash_attention_2\",\n",
    "        \"torch_dtype\": torch.bfloat16,\n",
    "        \"device_map\": \"auto\",\n",
    "    }\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(base_model, **model_kwargs)\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(lora_adapters_dir)\n",
    "\n",
    "    model = PeftModel.from_pretrained(base_model, lora_adapters_dir)\n",
    "    model = model.merge_and_unload()\n",
    "\n",
    "    model_size = sum(t.numel() for t in model.parameters())\n",
    "    print(f\"Merged Phi-3 model size: {model_size/1000**2:.1f}M parameters\")\n",
    "\n",
    "    # saving merged model for vLLM inference\n",
    "    model.save_pretrained(merged_model_folder)\n",
    "    tokenizer.save_pretrained(merged_model_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading merged/full model from /mnt/compatibility_checkpoints/actor/global_step_224\n",
      "INFO 03-29 14:13:47 config.py:1670] Downcasting torch.float32 to torch.float16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-29 14:13:51 arg_utils.py:963] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.\n",
      "INFO 03-29 14:13:51 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='/mnt/compatibility_checkpoints/actor/global_step_224', speculative_config=None, tokenizer='/mnt/compatibility_checkpoints/actor/global_step_224', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/mnt/compatibility_checkpoints/actor/global_step_224, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "INFO 03-29 14:13:52 selector.py:247] Cannot use FlashAttention-2 backend due to sliding window.\n",
      "INFO 03-29 14:13:52 selector.py:115] Using XFormers backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/zero/lib/python3.9/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "/anaconda/envs/zero/lib/python3.9/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-29 14:14:06 model_runner.py:1060] Starting to load model /mnt/compatibility_checkpoints/actor/global_step_224...\n",
      "INFO 03-29 14:14:06 selector.py:247] Cannot use FlashAttention-2 backend due to sliding window.\n",
      "INFO 03-29 14:14:06 selector.py:115] Using XFormers backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  2.17it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.94it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.79it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.86it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-29 14:14:09 model_runner.py:1071] Loading model weights took 7.1659 GB\n",
      "INFO 03-29 14:14:15 gpu_executor.py:122] # GPU blocks: 8999, # CPU blocks: 682\n",
      "INFO 03-29 14:14:15 gpu_executor.py:126] Maximum concurrency for 131072 tokens per request: 1.10x\n",
      "INFO 03-29 14:14:17 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-29 14:14:17 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 03-29 14:14:30 model_runner.py:1530] Graph capturing finished in 12 secs.\n"
     ]
    }
   ],
   "source": [
    "if use_lora_adapter:\n",
    "    print(f\"Loading base model: {base_model}\")\n",
    "    vllm_model = LLM(model=base_model, enable_lora=True)\n",
    "else:\n",
    "    print(f\"Loading merged/full model from {merged_model_folder}\")\n",
    "    vllm_model = LLM(model=merged_model_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vllm_inference(messages, vllm_model, **kwargs):\n",
    "    max_new_tokens = kwargs.get('max_new_tokens', 512)\n",
    "    temperature = kwargs.get('temperature', 0.0)\n",
    "    adapter_path = kwargs.get('adapter_path', None)\n",
    "\n",
    "    sampling_params = SamplingParams(max_tokens=max_new_tokens, temperature=temperature)\n",
    "\n",
    "    if type(messages) is not list:\n",
    "        messages = [messages]\n",
    "    if adapter_path is not None:\n",
    "        # https://docs.vllm.ai/en/latest/features/lora.html\n",
    "        outputs = vllm_model.chat(messages, sampling_params,\n",
    "                                  lora_request=LoRARequest(\"adapter\", 1, lora_path=adapter_path))\n",
    "    else:\n",
    "        outputs = vllm_model.chat(messages, sampling_params)\n",
    "    results = [o.outputs[0].text for o in outputs]\n",
    "    return results\n",
    "\n",
    "# Test\n",
    "# sampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n",
    "# prompts = [\"Hello, my name is\", \"The future of AI is\"]\n",
    "# outputs = vllm_model.generate(prompts, sampling_params)\n",
    "# for output in outputs:\n",
    "#     print(f\"Prompt: {output.prompt}, Generated text: {output.outputs[0].text}\")\n",
    "\n",
    "def inference(messages, model, tokenizer, **kwargs):\n",
    "    # messages = [\n",
    "    #     {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "    #     {\"role\": \"user\", \"content\": \"Can you provide ways to eat combinations of bananas and dragonfruits?\"},\n",
    "    #     {\"role\": \"assistant\", \"content\": \"Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey.\"},\n",
    "    #     {\"role\": \"user\", \"content\": \"What about solving an 2x + 3 = 7 equation?\"},\n",
    "    # ]\n",
    "    # device = torch.device('cuda:0')\n",
    "    # model.to(device)\n",
    "    max_new_tokens = kwargs.get('max_new_tokens', 512)\n",
    "    temperature = kwargs.get('temperature', 0.0)\n",
    "    # https://github.com/huggingface/transformers/blob/main/src/transformers/pipelines/base.py\n",
    "    # vi ~/anaconda3/envs/transformer-4-44-2-v2/lib/python3.10/site-packages/transformers/pipelines/base.py\n",
    "    # Line # 975, commented, Abir\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    generation_args = {\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        \"return_full_text\": False,\n",
    "        \"temperature\": temperature,\n",
    "        \"do_sample\": False,\n",
    "    }\n",
    "    if type(messages) is not list:\n",
    "        messages = [messages]\n",
    "    output = pipe(messages, **generation_args)\n",
    "    return output[0]['generated_text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating test split: 2894 examples [00:00, 55499.85 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['item_ids_original', 'item_ids_mapped', 'split', 'num_items', 'answer', 'messages', 'num_tokens'],\n",
       "        num_rows: 2894\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_files = {\"test\": os.path.join(data_dir, sft_test_file)}\n",
    "test_dataset = load_dataset(\"json\", data_files=data_files)\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Inference (vLLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2894 prompts ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   6%|▋         | 188/2894 [00:12<02:07, 21.24it/s, est. speed input: 9574.07 toks/s, output: 35.37 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-29 14:16:02 scheduler.py:1484] Sequence group 366 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2894/2894 [03:04<00:00, 15.68it/s, est. speed input: 11322.40 toks/s, output: 1157.24 toks/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# index = 0\n",
    "# example = test_dataset['test'][index]\n",
    "# res = inference(example['messages'][0], model, tokenizer)\n",
    "# we need to pass a list of lists, a singe list will be regarded as a single example\n",
    "prompts = [[example['messages'][0]] for example in test_dataset['test']]\n",
    "targets = [example['messages'][1]['content'] for example in test_dataset['test']]\n",
    "print(f\"Processing {len(prompts)} prompts ...\")\n",
    "\n",
    "if use_lora_adapter:\n",
    "    results = vllm_inference(prompts, vllm_model=vllm_model, adapter_path=lora_adapters_dir)\n",
    "else:\n",
    "    results = vllm_inference(prompts, vllm_model=vllm_model)\n",
    "# sampling_params = SamplingParams(max_tokens=256, temperature=0.0)\n",
    "# outputs = vllm_model.chat(prompts, sampling_params)\n",
    "# results = [o.outputs[0].text for o in outputs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "token_provider = get_bearer_token_provider(\n",
    "    DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\"\n",
    ")\n",
    "client = AzureOpenAI(\n",
    "    api_version=\"2024-02-15-preview\",\n",
    "    azure_endpoint=\"https://abir-openai.openai.azure.com/\",\n",
    "    azure_ad_token_provider=token_provider\n",
    ")\n",
    "\n",
    "def call_gpt_judge(client, **kwargs):\n",
    "    \"\"\"Use GPT to evaluate the model's output against the target.\"\"\"\n",
    "    generator_model = kwargs.get(\"generator_model\", None)\n",
    "    output_length = kwargs.get(\"output_length\", 32)\n",
    "    temperature = kwargs.get(\"temperature\", 0.0)\n",
    "    system_query = kwargs.get(\"system_query\", None)\n",
    "    eval_query = kwargs.get(\"eval_query\", None)\n",
    "\n",
    "    payload = [{\"role\":\"system\", \"content\": system_query},\n",
    "                    {\"role\":\"user\", \"content\": eval_query}]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "                model=generator_model,  # change from engine to model\n",
    "                messages=payload,\n",
    "                temperature=temperature,\n",
    "                max_tokens=output_length,\n",
    "                frequency_penalty=0,\n",
    "                presence_penalty=0,\n",
    "                stop=None)\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "system_query_evaluator = \"\"\"You are a helpful AI assistant that evaluates whether the generated response means `compatible` or `incompatible`.\n",
    "The text may not say `compatible` or `incompatible` explicitly, but you should infer it based on the content of the text. It may say in terms\n",
    "of whether the products can be worn together or not, or some other ways that imply compatibility or incompatibility.\n",
    "\n",
    "Return your answer with a single word: `compatible` or `incompatible`.\n",
    "Do not provide any additional explanation or text.\n",
    "\"\"\"\n",
    "\n",
    "# Example of how to call the GPT judge\n",
    "def return_judge_response(results):\n",
    "    \"\"\"Return the GPT judge response for each result.\"\"\"\n",
    "    gpt_judge_responses = []\n",
    "    for i in tqdm(range(len(results))):\n",
    "        eval_query = f\"Evaluate the following generated text for compatibility: {results[i]}\"\n",
    "        try:\n",
    "            response = call_gpt_judge(client,\n",
    "                                      generator_model=\"gpt-4o\",\n",
    "                                      output_length=5,\n",
    "                                      temperature=0.0,\n",
    "                                      system_query=system_query_evaluator,\n",
    "                                      eval_query=eval_query)\n",
    "            gpt_judge_responses.append(response.strip())\n",
    "        except Exception as e:\n",
    "            print(f\"Error calling GPT judge: {e}\")\n",
    "            gpt_judge_responses.append(\"error\")\n",
    "    return gpt_judge_responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:  incompatible\n",
      "GPT Judge Response: incompatible\n",
      "--------------------------------------------------\n",
      "Generated Text:  compatible\n",
      "GPT Judge Response: compatible\n",
      "--------------------------------------------------\n",
      "Generated Text:  incompatible\n",
      "GPT Judge Response: incompatible\n",
      "--------------------------------------------------\n",
      "Generated Text:  incompatible. The products in this outfit cannot be worn together due to their contrasting styles, colors, and levels of formality. The casual and bold floral bucket bag clashes with the elegant and glamorous gold tall boots, creating a mismatch in overall look and feel. Additionally, the bold and bold red A-line skirts further add to the clash, making it difficult to create a cohesive and harmonious ensemble. Each item is designed for different occasions and seasons, further highlighting their incompatibility when combined.\n",
      "GPT Judge Response: incompatible\n",
      "--------------------------------------------------\n",
      "Generated Text:  incompatible\n",
      "GPT Judge Response: incompatible\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "gpt_judge_outputs = return_judge_response(results)\n",
    "# Print the first 5 outputs for verification\n",
    "for i in range(5):\n",
    "    print(f\"Generated Text: {results[i]}\")\n",
    "    print(f\"GPT Judge Response: {gpt_judge_outputs[i]}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 69.52\n"
     ]
    }
   ],
   "source": [
    "df_grpo = pd.DataFrame({'gt': targets, 'predicted': gpt_judge_outputs})\n",
    "df_grpo['yhat'] = df_grpo['predicted'].apply(lambda x: x.split()[0].strip().strip('.').lower())\n",
    "df_grpo['gt'] = df_grpo['gt'].apply(lambda x: x.strip().strip('.').lower())\n",
    "df_grpo['acc'] = df_grpo.apply(lambda row: row['gt'].lower() == row['yhat'].lower(), axis=1)\n",
    "print(f\"Accuracy: {df_grpo['acc'].mean()*100:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>yhat</th>\n",
       "      <th>compatible</th>\n",
       "      <th>incompatible</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>compatible</th>\n",
       "      <td>817</td>\n",
       "      <td>496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>incompatible</th>\n",
       "      <td>386</td>\n",
       "      <td>1195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "yhat          compatible  incompatible\n",
       "y                                     \n",
       "compatible           817           496\n",
       "incompatible         386          1195"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_grpo['yhat'].value_counts()\n",
    "pd.crosstab(df_grpo['gt'], df_grpo['yhat'], rownames=['y'], colnames=['yhat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_solution(solution_str):\n",
    "    \"\"\"Extract the equation from the solution string.\"\"\"\n",
    "    answer_pattern = r'<answer>(.*?)</answer>'\n",
    "    match = re.finditer(answer_pattern, solution_str)\n",
    "    matches = list(match)\n",
    "    if matches:\n",
    "        final_answer = matches[-1].group(1).strip()\n",
    "    else:\n",
    "        final_answer = None\n",
    "    return final_answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating test split: 2894 examples [00:00, 87388.88 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['item_ids_original', 'item_ids_mapped', 'split', 'num_items', 'messages', 'num_tokens'],\n",
      "        num_rows: 2894\n",
      "    })\n",
      "})\n",
      "Processing 2894 prompts ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2894/2894 [03:05<00:00, 15.62it/s, est. speed input: 11280.54 toks/s, output: 1152.96 toks/s]\n"
     ]
    }
   ],
   "source": [
    "cot_sft_test_file = f\"{output_string}_cot_sft_test_2894.jsonl\"\n",
    "\n",
    "data_files_cot = {\"test\": os.path.join(data_dir, cot_sft_test_file)}\n",
    "test_dataset_cot = load_dataset(\"json\", data_files=data_files_cot)\n",
    "print(test_dataset_cot)\n",
    "\n",
    "prompts_cot = [[example['messages'][0]] for example in test_dataset_cot['test']]\n",
    "targets_cot = [extract_solution(example['messages'][1]['content']) for example in test_dataset_cot['test']]\n",
    "print(f\"Processing {len(prompts_cot)} prompts ...\")\n",
    "\n",
    "if use_lora_adapter:\n",
    "    results_cot = vllm_inference(prompts, vllm_model=vllm_model, adapter_path=lora_adapters_dir)\n",
    "else:\n",
    "    results_cot = vllm_inference(prompts, vllm_model=vllm_model)\n",
    "# sampling_params = SamplingParams(max_tokens=512, temperature=0.0)\n",
    "# outputs_cot = vllm_model.chat(prompts, sampling_params)\n",
    "# results_cot = [o.outputs[0].text for o in outputs_cot]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (CoT prompt): 45.16\n"
     ]
    }
   ],
   "source": [
    "df_grpo_cot = pd.DataFrame({'gt': targets_cot, 'predicted': results_cot})\n",
    "df_grpo_cot['yhat'] = df_grpo_cot['predicted'].apply(lambda x: x.split()[0].strip().strip('.').lower())\n",
    "df_grpo_cot['gt'] = df_grpo_cot['gt'].apply(lambda x: x.strip().strip('.').lower())\n",
    "\n",
    "df_grpo_cot['acc'] = df_grpo_cot.apply(lambda row: row['gt'].lower() == row['yhat'].lower(), axis=1)\n",
    "print(f\"Accuracy (CoT prompt): {df_grpo_cot['acc'].mean()*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>yhat</th>\n",
       "      <th>compatible</th>\n",
       "      <th>incompatible</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>compatible</th>\n",
       "      <td>480</td>\n",
       "      <td>453</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>incompatible</th>\n",
       "      <td>185</td>\n",
       "      <td>827</td>\n",
       "      <td>569</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "yhat          compatible  incompatible  the\n",
       "y                                          \n",
       "compatible           480           453  380\n",
       "incompatible         185           827  569"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(df_grpo_cot['gt'], df_grpo_cot['yhat'], rownames=['y'], colnames=['yhat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gt\n",
       "incompatible    0.546303\n",
       "compatible      0.453697\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_grpo['gt'].value_counts(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_dir = \"/home/azureuser/localfiles/TinyZero/checkpoints/compatibility/cp-phi-3.5/actor/global_step_120\"\n",
    "# device_map = 'cuda'\n",
    "# model_kwargs = dict(\n",
    "#     use_cache=False,\n",
    "#     trust_remote_code=True,\n",
    "#     attn_implementation=\"flash_attention_2\",  # loading the model with flash-attenstion support\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     device_map=device_map,\n",
    "# )\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_dir, **model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zero",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
