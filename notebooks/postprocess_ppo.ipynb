{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load a Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/zero/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-03-08 01:21:46,437\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "/anaconda/envs/zero/lib/python3.9/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:\n",
      "No module named 'vllm._version'\n",
      "  from vllm.version import __version__ as VLLM_VERSION\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers Version: 4.47.1\n",
      "Torch Version: 2.4.0+cu121\n",
      "4 CUDA (12.1) device available\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    pipeline)\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from peft import AutoPeftModelForCausalLM, PeftModel\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.lora.request import LoRARequest\n",
    "\n",
    "print(f\"Transformers Version: {transformers.__version__}\")\n",
    "print(f\"Torch Version: {torch.__version__}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"{torch.cuda.device_count()} CUDA ({torch.version.cuda}) device available\")\n",
    "else:\n",
    "    print(\"No CUDA device available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lora_adapters_dir = \"/home/azureuser/localfiles/models/frosty_apple_5d9xqfl2lw\"\n",
    "lora_adapters_dir = \"/home/azureuser/localfiles/models/grpo\"\n",
    "# merged_model_folder = \"/home/azureuser/localfiles/models/frosty_apple_5d9xqfl2lw_merged\"\n",
    "merged_model_folder = '/mnt/compatibility_checkpoints/actor/global_step_224'\n",
    "output_string = 'compatibility_text_based'\n",
    "sft_test_file = f\"{output_string}_sft_test_2894.jsonl\"\n",
    "data_dir = \"/home/azureuser/localfiles/data/polyvore_cp\"\n",
    "\n",
    "save_model = False\n",
    "use_lora_adapter = False\n",
    "base_model = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "# base_model = \"microsoft/Phi-3-mini-128k-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_model:\n",
    "\n",
    "    # Load the base model\n",
    "    # Determine if GPU is available\n",
    "    if torch.cuda.is_available():\n",
    "        device_map = 'cuda'\n",
    "        torch_dtype = torch.float16\n",
    "    else:\n",
    "        device_map = 'cpu'\n",
    "        torch_dtype = torch.bfloat16\n",
    "\n",
    "    model_kwargs = {\n",
    "        \"use_cache\": False,\n",
    "        \"trust_remote_code\": True,\n",
    "        \"attn_implementation\": \"flash_attention_2\",\n",
    "        \"torch_dtype\": torch.bfloat16,\n",
    "        \"device_map\": \"auto\",\n",
    "    }\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(base_model, **model_kwargs)\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(lora_adapters_dir)\n",
    "\n",
    "    model = PeftModel.from_pretrained(base_model, lora_adapters_dir)\n",
    "    model = model.merge_and_unload()\n",
    "\n",
    "    model_size = sum(t.numel() for t in model.parameters())\n",
    "    print(f\"Merged Phi-3 model size: {model_size/1000**2:.1f}M parameters\")\n",
    "\n",
    "    # saving merged model for vLLM inference\n",
    "    model.save_pretrained(merged_model_folder)\n",
    "    tokenizer.save_pretrained(merged_model_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-08 01:22:45 config.py:1670] Downcasting torch.float32 to torch.float16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-08 01:22:58 arg_utils.py:963] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.\n",
      "INFO 03-08 01:22:58 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='/mnt/compatibility_checkpoints/actor/global_step_224', speculative_config=None, tokenizer='/mnt/compatibility_checkpoints/actor/global_step_224', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/mnt/compatibility_checkpoints/actor/global_step_224, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "INFO 03-08 01:22:58 selector.py:247] Cannot use FlashAttention-2 backend due to sliding window.\n",
      "INFO 03-08 01:22:58 selector.py:115] Using XFormers backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/zero/lib/python3.9/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "/anaconda/envs/zero/lib/python3.9/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-08 01:23:06 model_runner.py:1060] Starting to load model /mnt/compatibility_checkpoints/actor/global_step_224...\n",
      "INFO 03-08 01:23:06 selector.py:247] Cannot use FlashAttention-2 backend due to sliding window.\n",
      "INFO 03-08 01:23:06 selector.py:115] Using XFormers backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.77it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  2.76it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.66it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.64it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.74it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-08 01:23:09 model_runner.py:1071] Loading model weights took 7.1659 GB\n",
      "INFO 03-08 01:23:15 gpu_executor.py:122] # GPU blocks: 8999, # CPU blocks: 682\n",
      "INFO 03-08 01:23:15 gpu_executor.py:126] Maximum concurrency for 131072 tokens per request: 1.10x\n",
      "INFO 03-08 01:23:18 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-08 01:23:18 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 03-08 01:23:31 model_runner.py:1530] Graph capturing finished in 13 secs.\n"
     ]
    }
   ],
   "source": [
    "if use_lora_adapter:\n",
    "    print(f\"Loading base model: {base_model}\")\n",
    "    vllm_model = LLM(model=base_model, enable_lora=True)\n",
    "else:\n",
    "    print(f\"Loading merged/full model from {merged_model_folder}\")\n",
    "    vllm_model = LLM(model=merged_model_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vllm_inference(messages, vllm_model, **kwargs):\n",
    "    max_new_tokens = kwargs.get('max_new_tokens', 512)\n",
    "    temperature = kwargs.get('temperature', 0.0)\n",
    "    adapter_path = kwargs.get('adapter_path', None)\n",
    "\n",
    "    sampling_params = SamplingParams(max_tokens=max_new_tokens, temperature=temperature)\n",
    "\n",
    "    if type(messages) is not list:\n",
    "        messages = [messages]\n",
    "    if adapter_path is not None:\n",
    "        # https://docs.vllm.ai/en/latest/features/lora.html\n",
    "        outputs = vllm_model.chat(messages, sampling_params,\n",
    "                                  lora_request=LoRARequest(\"adapter\", 1, lora_path=adapter_path))\n",
    "    else:\n",
    "        outputs = vllm_model.chat(messages, sampling_params)\n",
    "    results = [o.outputs[0].text for o in outputs]\n",
    "    return results\n",
    "\n",
    "# Test\n",
    "# sampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n",
    "# prompts = [\"Hello, my name is\", \"The future of AI is\"]\n",
    "# outputs = vllm_model.generate(prompts, sampling_params)\n",
    "# for output in outputs:\n",
    "#     print(f\"Prompt: {output.prompt}, Generated text: {output.outputs[0].text}\")\n",
    "\n",
    "def inference(messages, model, tokenizer, **kwargs):\n",
    "    # messages = [\n",
    "    #     {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "    #     {\"role\": \"user\", \"content\": \"Can you provide ways to eat combinations of bananas and dragonfruits?\"},\n",
    "    #     {\"role\": \"assistant\", \"content\": \"Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey.\"},\n",
    "    #     {\"role\": \"user\", \"content\": \"What about solving an 2x + 3 = 7 equation?\"},\n",
    "    # ]\n",
    "    # device = torch.device('cuda:0')\n",
    "    # model.to(device)\n",
    "    max_new_tokens = kwargs.get('max_new_tokens', 512)\n",
    "    temperature = kwargs.get('temperature', 0.0)\n",
    "    # https://github.com/huggingface/transformers/blob/main/src/transformers/pipelines/base.py\n",
    "    # vi ~/anaconda3/envs/transformer-4-44-2-v2/lib/python3.10/site-packages/transformers/pipelines/base.py\n",
    "    # Line # 975, commented, Abir\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    generation_args = {\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        \"return_full_text\": False,\n",
    "        \"temperature\": temperature,\n",
    "        \"do_sample\": False,\n",
    "    }\n",
    "    if type(messages) is not list:\n",
    "        messages = [messages]\n",
    "    output = pipe(messages, **generation_args)\n",
    "    return output[0]['generated_text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['item_ids_original', 'item_ids_mapped', 'split', 'num_items', 'answer', 'messages', 'num_tokens'],\n",
       "        num_rows: 2894\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_files = {\"test\": os.path.join(data_dir, sft_test_file)}\n",
    "test_dataset = load_dataset(\"json\", data_files=data_files)\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Inference (vLLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2894 prompts ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2894/2894 [01:42<00:00, 28.25it/s, est. speed input: 20397.26 toks/s, output: 162.72 toks/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# index = 0\n",
    "# example = test_dataset['test'][index]\n",
    "# res = inference(example['messages'][0], model, tokenizer)\n",
    "# we need to pass a list of lists, a singe list will be regarded as a single example\n",
    "prompts = [[example['messages'][0]] for example in test_dataset['test']]\n",
    "targets = [example['messages'][1]['content'] for example in test_dataset['test']]\n",
    "print(f\"Processing {len(prompts)} prompts ...\")\n",
    "\n",
    "if use_lora_adapter:\n",
    "    results = vllm_inference(prompts, vllm_model=vllm_model, adapter_path=lora_adapters_dir)\n",
    "else:\n",
    "    results = vllm_inference(prompts, vllm_model=vllm_model)\n",
    "# sampling_params = SamplingParams(max_tokens=256, temperature=0.0)\n",
    "# outputs = vllm_model.chat(prompts, sampling_params)\n",
    "# results = [o.outputs[0].text for o in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 63.58\n"
     ]
    }
   ],
   "source": [
    "df_grpo = pd.DataFrame({'gt': targets, 'predicted': results})\n",
    "df_grpo['yhat'] = df_grpo['predicted'].apply(lambda x: x.split()[0].strip().strip('.').lower())\n",
    "df_grpo['gt'] = df_grpo['gt'].apply(lambda x: x.strip().strip('.').lower())\n",
    "df_grpo['acc'] = df_grpo.apply(lambda row: row['gt'].lower() == row['yhat'].lower(), axis=1)\n",
    "print(f\"Accuracy: {df_grpo['acc'].mean()*100:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>yhat</th>\n",
       "      <th>compatible</th>\n",
       "      <th>incompatible</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>compatible</th>\n",
       "      <td>475</td>\n",
       "      <td>838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>incompatible</th>\n",
       "      <td>216</td>\n",
       "      <td>1365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "yhat          compatible  incompatible\n",
       "y                                     \n",
       "compatible           475           838\n",
       "incompatible         216          1365"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_grpo['yhat'].value_counts()\n",
    "pd.crosstab(df_grpo['gt'], df_grpo['yhat'], rownames=['y'], colnames=['yhat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_solution(solution_str):\n",
    "    \"\"\"Extract the equation from the solution string.\"\"\"\n",
    "    answer_pattern = r'<answer>(.*?)</answer>'\n",
    "    match = re.finditer(answer_pattern, solution_str)\n",
    "    matches = list(match)\n",
    "    if matches:\n",
    "        final_answer = matches[-1].group(1).strip()\n",
    "    else:\n",
    "        final_answer = None\n",
    "    return final_answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['item_ids_original', 'item_ids_mapped', 'split', 'num_items', 'messages', 'num_tokens'],\n",
      "        num_rows: 2894\n",
      "    })\n",
      "})\n",
      "Processing 2894 prompts ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2894/2894 [01:43<00:00, 27.91it/s, est. speed input: 20150.93 toks/s, output: 160.75 toks/s]\n"
     ]
    }
   ],
   "source": [
    "cot_sft_test_file = f\"{output_string}_cot_sft_test_2894.jsonl\"\n",
    "\n",
    "data_files_cot = {\"test\": os.path.join(data_dir, cot_sft_test_file)}\n",
    "test_dataset_cot = load_dataset(\"json\", data_files=data_files_cot)\n",
    "print(test_dataset_cot)\n",
    "\n",
    "prompts_cot = [[example['messages'][0]] for example in test_dataset_cot['test']]\n",
    "targets_cot = [extract_solution(example['messages'][1]['content']) for example in test_dataset_cot['test']]\n",
    "print(f\"Processing {len(prompts_cot)} prompts ...\")\n",
    "\n",
    "if use_lora_adapter:\n",
    "    results_cot = vllm_inference(prompts, vllm_model=vllm_model, adapter_path=lora_adapters_dir)\n",
    "else:\n",
    "    results_cot = vllm_inference(prompts, vllm_model=vllm_model)\n",
    "# sampling_params = SamplingParams(max_tokens=512, temperature=0.0)\n",
    "# outputs_cot = vllm_model.chat(prompts, sampling_params)\n",
    "# results_cot = [o.outputs[0].text for o in outputs_cot]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (CoT prompt): 63.58\n"
     ]
    }
   ],
   "source": [
    "df_grpo_cot = pd.DataFrame({'gt': targets_cot, 'predicted': results_cot})\n",
    "df_grpo_cot['yhat'] = df_grpo_cot['predicted'].apply(lambda x: x.split()[0].strip().strip('.').lower())\n",
    "df_grpo_cot['gt'] = df_grpo_cot['gt'].apply(lambda x: x.strip().strip('.').lower())\n",
    "\n",
    "df_grpo_cot['acc'] = df_grpo_cot.apply(lambda row: row['gt'].lower() == row['yhat'].lower(), axis=1)\n",
    "print(f\"Accuracy (CoT prompt): {df_grpo_cot['acc'].mean()*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>yhat</th>\n",
       "      <th>compatible</th>\n",
       "      <th>incompatible</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>compatible</th>\n",
       "      <td>475</td>\n",
       "      <td>838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>incompatible</th>\n",
       "      <td>216</td>\n",
       "      <td>1365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "yhat          compatible  incompatible\n",
       "y                                     \n",
       "compatible           475           838\n",
       "incompatible         216          1365"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(df_grpo_cot['gt'], df_grpo_cot['yhat'], rownames=['y'], colnames=['yhat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gt\n",
       "incompatible    0.546303\n",
       "compatible      0.453697\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_grpo['gt'].value_counts(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_dir = \"/home/azureuser/localfiles/TinyZero/checkpoints/compatibility/cp-phi-3.5/actor/global_step_120\"\n",
    "# device_map = 'cuda'\n",
    "# model_kwargs = dict(\n",
    "#     use_cache=False,\n",
    "#     trust_remote_code=True,\n",
    "#     attn_implementation=\"flash_attention_2\",  # loading the model with flash-attenstion support\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     device_map=device_map,\n",
    "# )\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_dir, **model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zero",
   "language": "python",
   "name": "zero"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
